**Prompts:**
*How to technical tools promise to fair out the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?*

*Imagine, what could this (following quote) mean in the widest sense? "The state doesn't need a cop to kill a person" and "electronic incarceration"*

*What do you understand this to mean? "systems act as a kind of 'empathy-overwrite'"*

*China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of "technical systems not well thought-through about what their impact on human beings is"*

**Response:**


**Notes:**

Notes on Trevor Paglen video
white guy
mass surveillance is no longer about just security cameras that are recording, but what happens when they record?
"all of the unacknowledged"? just forgotten about or secret? (both?)
images of what computer vision is seeing (us-mexico border example)
Police abusing machine learning for ease rather than actual work
Grocery store reminds me of the Target pregnancy example
Technology is always used, so there's no way for it t0 be neutral
public wifi encryption with basics of vpn, but even that requires login


Notes on Joy Buolamwini video
coded gaze
Aspire Mirror - snapchat filter, even her own data was biased to not be able to detect her own face
Her face couldn't be used for her own assignment and had to use her roommate's
the "general data" wasn't general enough, selection bias
To what extent do civil liberties become breached with the use of predictive policing and cameras being used to identify suspects or innocent civilians
Widespread mysterious and destructive algorithms - once again disproportionately affecting minorities
inclusive coding - are blindspots being checked? no
think of the impact of your code

Notes on Machine Learning and Human Bias video
Programmers expose their own bias in the programs they write
interaction bias - as the program interacts based on how people interact with it forms the bias
latent bias - using the past to predict the future without acknowledging social reasons
selection bias - the model the program was trained on was given a selection too small, therefore the bias focuses on this small selection
Promotion to show that their products are meant for good, but they need help to improve the bias of their products with the help from users like "us"

Notes on Automating Inequality podcast
"Digital poorhouse" - more evolution than revolution for tools. 1819- decided to raise barriers for receiving public aid, basically incarcerated those who entered the poorhouse like giving up their rights like voting, marriage, etc. Predictive policing, data analysis becomes the digital poorhouse as they selectively choose who actually needs the aid.

intention of the designers is good and committed, transparent, accountability, participatory design

Indiana Welfare Automation
decides who is able to receive SNAP, Medicare, and other benefits
Indiana v. IBM, about a million benefits denial following the implement of the program
relationship between the social worker and the family, move from casework (building relationship) to task-based system, made the benefit denial inhuman, with no alternative available, if a mistake was made it was on the family no longer on the case worker

Coordinated Entry (match.com of homeless services) in LA County
most homeless to most accessible available resources
all three criteria met
58000 people homeless, tried to prioritize with a lack of empathy, there's just not enough of affordable housing

Alleganie PA Family Screening tools
Which children are most likely to be neglected or abused
all three criteria met
Supposed to predict people's actions in the future, dehumanized those being used for the data, just a system of data points, does not take into account the parenting while poor effect and not that there has to be a correlation between the two, will the risk score follow them and their family members?

even the best technology tools have lead to disparities
you don't need AI or machine learning to make smart decisions about our policies
rethink who the experts are for writing the programs for the machine learning algorithm, should take into account actual people who are affected by the decisions being made and those who have previously been making the decisions

Notes on AI Bias podcast
AI is too broad now because it's been around for too long, so it's being applied to anything and everything
Machine learning is the infrastructure for your technological life, as it's in everything
Reminds me of the concept of Siri because people don't think about the information being recorded all the time, when they referred to the "giant robots are going to take over the world, but we're too stupid to realize that"

Predictive profiling is not without bias as those creating the algorithms are also the same kind of people who did redlining (mortgage example)
Previous racial bias become the data being used for the machine learning algorithm, with the computer "lack of feelings" justifies the companies to use this system, however the system continues to be unfair (because there's no recent when using past data)
Predictive profiling needs to be socio-technical, not just technological (Automating inequality reminder)

If homogenous communities are creating a worldview, than the concentration will be narrow and only value the same qualities to benefit the homogenous community of the room
interdisciplinary AI reminds me of the use of IMA vs computer science, as the IMA department definitely has students with strong coding backgrounds, but add a second discipline of creating experience for a user, and thinking about the audience first, rather than the outcome

Is re-training the only thing required, or is new data required? re-training with the same data will have no effect on the outcome, so new data from medical studies are needed to actually change for a better system

The birds work for the bourgeoisie (Alexa in the home)
If one checks in, the network is also giving the data, data collection becomes not an individual issue but a structural issue

How do you regulate AI and systems that we have to rely on every day?

It's almost like policing is racially biased and the US should understand this from the beginning if not from the way predictive policing works
AI impacts through forensics fairness could lead to a more complete story for machine learning to come out with better sorting
If companies could be required to hire sociologist or psychologists and/or social researchers in order to control the impact that the use of these systems have, but currently the capitalist system keeps this from happening as money is worth more than morality
Just because you can do it, doesn't mean you should do it
