**Prompts:**

*How to technical tools promise to fair out the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?*

*Imagine, what could this (following quote) mean in the widest sense? "The state doesn't need a cop to kill a person" and "electronic incarceration"*

*What do you understand this to mean? "systems act as a kind of 'empathy-overwrite'"*

*China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of "technical systems not well thought-through about what their impact on human beings is"*

**Response:**

Technical tools promise to make handing out benefits to those who need it more fair, however the data being used to train these models severely affect the outcome of the distribution. Often, these technical tools are deemed to be fair because they are computers with a lack of feelings, but often this lack of feeling also leads to disastrous effects of not seeing the social situation that would make a better decision. For example, in the Alleganie Family Screening tools, the algorithm was meant to find children that were in families that were more likely to abuse or neglect them. However, the algorithm began choosing only those living below the poverty line. While this algorithm was able to predict that these children were likely to be neglected, it did not consider whether or not the neglect was on purpose, or because of socioeconomic factors. This is also an example for the third prompt of "systems act as a kind of 'empathy overwrite'". The computer algorithm completely bulldozed the conditions in which the family is in. It failed to consider whether or not the family cares for the child and attempts to take care of their child. In addition, this lack of feeling also left many stranded in the Indiana Welfare Automation case. The algorithm determined who "really" needed the welfare, but millions were denied their benefits that did need the benefits to keep their family taken care of. Instead of being able to discuss with a social worker about the issue with a lack of access to welfare, this new system eliminated the relationship-based system of the family and the social worker and became inhumane with no chance for discrepancy.

China has been expansive in the quest for using data to make life more convenient. Even so, problems arise with these algorithms with selection bias. For example, instead of having to have police patrolling the night and streets in their car, they can be sat while watching the surveillance cameras of each street and the station can let them know exactly where an incident is occurring. While this may seem money and life-saving, this also runs into issues as not all streets are surveilled so closely. When a drunk man was beating a woman outside the gates of the Jinqiao community (NYU Shanghai residence halls), when the police were called, they decided they would not respond to the situation because they could not see anything happen on the street. The incident instead had to be stopped by Resident Assistants of the residence hall and security guards of the community, putting their safety in danger. This system really did not consider the impact this could have if not every single street is so closely watched, the impact on the humans involved. How much more incidents are happening because of this selection bias?

**Notes:**

Notes on Trevor Paglen video
* white guy
* mass surveillance is no longer about just security cameras that are recording, but what happens when they record?
* "all of the unacknowledged"? just forgotten about or secret? (both?)
* images of what computer vision is seeing (us-mexico border example)
* Police abusing machine learning for ease rather than actual work
* Grocery store reminds me of the Target pregnancy example
* Technology is always used, so there's no way for it t0 be neutral
* public wifi encryption with basics of vpn, but even that requires login


Notes on Joy Buolamwini video
* coded gaze
* Aspire Mirror - snapchat filter, even her own data was biased to not be able to detect her own face
* Her face couldn't be used for her own assignment and had to use her roommate's
* the "general data" wasn't general enough, selection bias
* To what extent do civil liberties become breached with the use of predictive policing and cameras being used to identify suspects or * innocent civilians
* Widespread mysterious and destructive algorithms - once again disproportionately affecting minorities
* inclusive coding - are blindspots being checked? no
* think of the impact of your code

Notes on Machine Learning and Human Bias video
* Programmers expose their own bias in the programs they write
* interaction bias - as the program interacts based on how people interact with it forms the bias
* latent bias - using the past to predict the future without acknowledging social reasons
* selection bias - the model the program was trained on was given a selection too small, therefore the bias focuses on this small selection
* Promotion to show that their products are meant for good, but they need help to improve the bias of their products with the help from users like "us"

Notes on Automating Inequality podcast
* "Digital poorhouse" - more evolution than revolution for tools. 1819- decided to raise barriers for receiving public aid, basically incarcerated those who entered the poorhouse like giving up their rights like voting, marriage, etc. Predictive policing, data analysis becomes the digital poorhouse as they selectively choose who actually needs the aid.

* intention of the designers is good and committed, transparent, accountability, participatory design

* Indiana Welfare Automation
  * decides who is able to receive SNAP, Medicare, and other benefits
  * Indiana v. IBM, about a million benefits denial following the implement of the program
  * relationship between the social worker and the family, move from casework (building relationship) to task-based system, made the benefit denial inhuman, with no alternative available, if a mistake was made it was on the family no longer on the case worker

* Coordinated Entry (match.com of homeless services) in LA County
  * most homeless to most accessible available resources
  * all three criteria met
  * 58000 people homeless, tried to prioritize with a lack of empathy, there's just not enough of affordable housing

* Alleganie PA Family Screening tools
  * Which children are most likely to be neglected or abused
  * all three criteria met
  * Supposed to predict people's actions in the future, dehumanized those being used for the data, just a system of data points, does not take into account the parenting while poor effect and not that there has to be a correlation between the two, will the risk score follow them and their family members?

* even the best technology tools have lead to disparities
* you don't need AI or machine learning to make smart decisions about our policies
* rethink who the experts are for writing the programs for the machine learning algorithm, should take into account actual people who are * affected by the decisions being made and those who have previously been making the decisions

Notes on AI Bias podcast
* AI is too broad now because it's been around for too long, so it's being applied to anything and everything
* Machine learning is the infrastructure for your technological life, as it's in everything
* Reminds me of the concept of Siri because people don't think about the information being recorded all the time, when they referred to the "giant robots are going to take over the world, but we're too stupid to realize that"

* Predictive profiling is not without bias as those creating the algorithms are also the same kind of people who did redlining (mortgage example)
* Previous racial bias become the data being used for the machine learning algorithm, with the computer "lack of feelings" justifies the companies to use this system, however the system continues to be unfair (because there's no recent when using past data)
* Predictive profiling needs to be socio-technical, not just technological (Automating inequality reminder)

* If homogenous communities are creating a worldview, than the concentration will be narrow and only value the same qualities to benefit the homogenous community of the room
* interdisciplinary AI reminds me of the use of IMA vs computer science, as the IMA department definitely has students with strong coding backgrounds, but add a second discipline of creating experience for a user, and thinking about the audience first, rather than the outcome

* Is re-training the only thing required, or is new data required? re-training with the same data will have no effect on the outcome, so new data from medical studies are needed to actually change for a better system

* The birds work for the bourgeoisie (Alexa in the home)
* If one checks in, the network is also giving the data, data collection becomes not an individual issue but a structural issue

* How do you regulate AI and systems that we have to rely on every day?

* It's almost like policing is racially biased and the US should understand this from the beginning if not from the way predictive policing works
* AI impacts through forensics fairness could lead to a more complete story for machine learning to come out with better sorting
* If companies could be required to hire sociologist or psychologists and/or social researchers in order to control the impact that the use of these systems have, but currently the capitalist system keeps this from happening as money is worth more than morality
* Just because you can do it, doesn't mean you should do it
